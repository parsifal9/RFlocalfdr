@article{Altmann.et.al.2010,
  author =	 {Altmann, Andr\'{e} and Tolo\c{s}i, Laura and Sander, Oliver and Lengauer, Thomas},
  title =	 {Permutation importance: a corrected feature importance measure},
  journal =	 {Bioinformatics},
  volume =	 26,
  number =	 10,
  pages =	 1340,
  year =	 2010,
  doi =		 {10.1093/bioinformatics/btq134},
  URL =		 { + http://dx.doi.org/10.1093/bioinformatics/btq134},
  eprint =	 {/oup/backfile/Content_public/Journal/bioinformatics/26/10/10.1093/bioinformatics/btq134/2/btq134.pdf}
}

@Manual{Azzalini.2022,
  title =	 {The {R} package \texttt{sn}: The skew-normal and related distributions such as the skew-$t$ and the
                  {SUN} (version 2.0.2).},
  author =	 {A. Azzalini},
  address =	 {Universit\`a degli Studi di Padova, Italia},
  year =	 2022,
  note =	 {Home page: \url{http://azzalini.stat.unipd.it/SN/}},
  url =		 {https://cran.r-project.org/package=sn},
}

@Article{	  Bayat.et.al.VariantSparkCloudbasedMachine2020,
  title =	 {{{VariantSpark}}: {{Cloud}}-Based Machine Learning for Association Study of Complex Phenotype and
                  Large-Scale Genomic Data},
  shorttitle =	 {{{VariantSpark}}},
  author =	 {Bayat, Arash and Szul, Piotr and O'Brien, Aidan R. and Dunne, Robert and Hosking, Brendan and Jain,
                  Yatish and Hosking, Cameron and Luo, Oscar J. and Twine, Natalie and Bauer, Denis C.},
  year =	 2020,
  month =	 10,
  volume =	 9,
  publisher =	 {{Oxford Academic}},
  doi =		 {10.1093/gigascience/giaa077},
  abstract =	 {AbstractBackground. Many traits and diseases are thought to be driven by \&gt;1 gene
                  (polygenic). Polygenic risk scores (PRS) hence expand on genome-wide associ},
  file =	 {/home/dun280/Dropbox/zotero/GigaScience/2020/Bayat et al_2020_VariantSpark.pdf},
  journal =	 {GigaScience},
  language =	 {en},
  number =	 8
}

@Article{	  Benjamini.and.Hochberg.1995,
  title =	 "Controlling the false discovery rate: a practical and powerful approach to multiple testing",
  author =	 "Yoav Benjamini and Yosef Hochberg",
  journal =	 "Journal of the Royal Statistical Society B",
  volume =	 57,
  number =	 1,
  pages =	 "289\-300",
  year =	 1995
}

@Conference{Broadbent.et.al.2019,
  author =	 {Broadbent, James and Stockwell, Sally and Byrne, Keren and Bose, Utpal and Ramm, Kerrie and Hyles,
                  Jessica and Trevaskis, Ben and Dillon, Shannon and Colgrave, Michelle},
  title =	 {{OzWheat - A genetic diversity panel for classification and prediction of wheat traits}},
  booktitle =	 {Human Proteome Organisation World Conference; Adelaide, Australia},
  year =	 2019,
  organization = {Human Proteome Organisation World Conference},
  note =	 {\url{http://hdl.handle.net/102.100.100/421985?index=1}}
}

@Article{Breiman.2001,
  author =	 {Leo Breiman},
  title =	 {Random Forests},
  journal =	 {Machine Learning},
  year =	 2001,
  volume =	 45,
  number =	 1,
  pages =	 {5-32},
  month =	 {October}
}

@Article{	  Churchill.and.Doerge.1994,
  author =	 {Churchill, G A and Doerge, R W},
  title =	 {Empirical threshold values for quantitative trait mapping.},
  volume =	 138,
  number =	 3,
  pages =	 {963--971},
  year =	 1994,
  publisher =	 {Genetics},
  abstract =	 {The detection of genes that control quantitative characters is a problem of great interest to the
                  genetic mapping community. Methods for locating these quantitative trait loci (QTL) relative to maps
                  of genetic markers are now widely used. This paper addresses an issue common to all QTL mapping
                  methods, that of determining an appropriate threshold value for declaring significant QTL effects. An
                  empirical method is described, based on the concept of a permutation test, for estimating threshold
                  values that are tailored to the experimental data at hand. The method is demonstrated using two real
                  data sets derived from F(2) and recombinant inbred plant populations. An example using simulated data
                  from a backcross design illustrates the effect of marker density on threshold values.},
  issn =	 {0016-6731},
  url =		 {http://www.genetics.org/content/138/3/963},
  eprint =	 {http://www.genetics.org/content/138/3/963.full.pdf},
  journal =	 {Genetics}
}

@Article{	  Cui2021.03.12.435063,
  title =	 {Gene-Gene Interaction Detection with Deep Learning},
  author =	 {Cui, Tianyu and El Mekkaoui, Khaoula and Reinvall, Jaakko and Havulinna, Aki S. and Marttinen, Pekka
                  and Kaski, Samuel},
  year =	 2021,
  publisher =	 {{Cold Spring Harbor Laboratory}},
  doi =		 {10.1101/2021.03.12.435063},
  abstract =	 {We do not know the extent to which genetic interactions affect the observed phenotype in diseases,
                  because the current interaction detection approaches are limited: they only consider interactions
                  between the top SNPs of each gene, and only simple forms of interaction. We introduce methods for
                  increasing the statistical power of interaction detection by taking into account all SNPs and complex
                  interactions between them, beyond only the currently considered multiplicative relationships. In
                  brief, the relation between SNPs and a phenotype is captured by a gene interaction neural network
                  (NN), and the interactions are quantified by the Shapley score between hidden nodes, which are gene
                  representations that optimally combine information from all SNPs in the gene. Additionally, we design
                  a new permutation procedure tailored for NNs to assess the significance of interactions. The new
                  approach outperformed existing alternatives on simulated datasets, and in a cholesterol study on the
                  UK Biobank it detected six interactions which replicated on an independent FINRISK dataset, four of
                  them novel findings.Competing Interest StatementThe authors have declared no competing interest.},
  elocation-id = {2021.03.12.435063},
  eprint =	 {https://www.biorxiv.org/content/early/2021/03/12/2021.03.12.435063.full.pdf},
  file =	 {/home/dun280/Dropbox/zotero/Cui et al/Cui et al_2021_Gene-gene interaction detection with deep
                  learning.pdf},
  journal =	 {bioRxiv}
}

@Article{	  Degenhardt.et.al.2019,
  title =	 {Evaluation of variable selection methods for random forests and omics data sets},
  volume =	 20,
  issn =	 {1477-4054},
  url =		 {https://academic.oup.com/bib/article/20/2/492/4554516},
  doi =		 {10.1093/bib/bbx124},
  language =	 {en},
  number =	 2,
  urldate =	 {2019-08-20},
  journal =	 {Briefings in Bioinformatics},
  author =	 {Degenhardt, Frauke and Seifert, Stephan and Szymczak, Silke},
  month =	 mar,
  year =	 2019,
  pages =	 {492--503},
  file =	 {Degenhardt et al. - 2019 - Evaluation of variable selection methods for
                  rando.pdf:/home/dun280/Dropbox/zotero/Briefings in Bioinformatics/2019/Degenhardt et al. - 2019 -
                  Evaluation of variable selection methods for rando2.pdf:application/pdf}
}

@article{Diaz-Uriarte.and.Andres.2006,
  title =	 {Gene selection and classification of microarray data using random forest},
  volume =	 7,
  issn =	 {1471-2105},
  url =		 {https://doi.org/10.1186/1471-2105-7-3},
  doi =		 {10.1186/1471-2105-7-3},
  abstract =	 {Selection of relevant genes for sample classification is a common task in most gene expression
                  studies, where researchers try to identify the smallest possible set of genes that can still achieve
                  good predictive performance (for instance, for future use with diagnostic purposes in clinical
                  practice). Many gene selection approaches use univariate (gene-by-gene) rankings of gene relevance and
                  arbitrary thresholds to select the number of genes, can only be applied to two-class problems, and use
                  gene selection ranking criteria unrelated to the classification algorithm. In contrast, random forest
                  is a classification algorithm well suited for microarray data: it shows excellent performance even
                  when most predictive variables are noise, can be used when the number of variables is much larger than
                  the number of observations and in problems involving more than two classes, and returns measures of
                  variable importance. Thus, it is important to understand the performance of random forest with
                  microarray data and its possible use for gene selection.},
  urldate =	 {2018-07-02},
  journal =	 {BMC Bioinformatics},
  author =	 {D\'iaz-Uriarte, Ram\'on and Alvarez de Andr\'es, Sara},
  month =	 jan,
  year =	 2006,
  keywords =	 {Gene Selection, Random Forest, Support Vector Machine, Variable Importance, Variable Selection},
  pages =	 3,
  file =	 {Snapshot:/home/dun280/local/zotero/storage/SWGJQQ25/1471-2105-7-3.html:text/html}
}

@InProceedings{Donoho.and.Stodden.2006,
  Title =	 {Breakdown point of model selection when the number of variables exceeds the number of observations},
  Author =	 {Donoho, David and Stodden, Victoria},
  Booktitle =	 {The 2006 IEEE International Joint Conference on Neural Network Proceedings},
  Year =	 2006,
  Organization = {IEEE},
  Pages =	 {1916-1921}
}

@Article{Donoho.and.Tanner.2009,
  Title =	 {Observed universality of phase transitions in high-dimensional geometry, with implications for modern
                  data analysis and signal processing},
  Author =	 {Donoho, David and Tanner, Jared},
  Journal =	 {Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering
                  Sciences},
  Year =	 2009,
  Number =	 1906,
  Pages =	 {4273-4293},
  Volume =	 367,
  Abstract =	 {We review connections between phase transitions in high-dimensional combinatorial geometry and phase
                  transitions occurring in modern high-dimensional data analysis and signal processing. In data
                  analysis, such transitions arise as abrupt breakdown of linear model selection, robust data fitting or
                  compressed sensing reconstructions, when the complexity of the model or the number of outliers
                  increases beyond a threshold. In combinatorial geometry, these transitions appear as abrupt changes in
                  the properties of face counts of convex polytopes when the dimensions are varied. The thresholds in
                  these very different problems appear in the same critical locations after appropriate calibration of
                  variables. These thresholds are important in each subject area: for linear modelling, they place hard
                  limits on the degree to which the now ubiquitous high-throughput data analysis can be successful; for
                  robustness, they place hard limits on the degree to which standard robust fitting methods can tolerate
                  outliers before breaking down; for compressed sensing, they define the sharp boundary of the
                  undersampling/sparsity trade-off curve in undersampling theorems. Existing derivations of phase
                  transitions in combinatorial geometry assume that the underlying matrices have independent and
                  identically distributed Gaussian elements. In applications, however, it often seems that Gaussianity
                  is not required. We conducted an extensive computational experiment and formal inferential analysis to
                  test the hypothesis that these phase transitions are universal across a range of underlying matrix
                  ensembles. We ran millions of linear programs using random matrices spanning several matrix ensembles
                  and problem sizes; visually, the empirical phase transitions do not depend on the ensemble, and they
                  agree extremely well with the asymptotic theory assuming Gaussianity. Careful statistical analysis
                  reveals discrepancies that can be explained as transient terms, decaying with problem size. The
                  experimental results are thus consistent with an asymptotic large-n universality across matrix
                  ensembles; finite-sample universality can be rejected. {\textcopyright} 2009 The Royal Society},
  Doi =		 {10.1098/rsta.2009.0152},
  Eprint =	 {http://rsta.royalsocietypublishing.org/content/367/1906/4273.full.pdf},
  ISSN =	 {1364-503X},
  Publisher =	 {The Royal Society},
  Url =		 {http://rsta.royalsocietypublishing.org/content/367/1906/4273}
}

@misc{Efron.2005,
  title =	 {Local {{False Discovery Rates}}},
  author =	 {Efron, Bradley},
  year =	 2005,
  file =	 {/home/dun280/Dropbox/zotero/Efron/Efron_2005_Local False Discovery Rates.pdf},
  eprint =	 {\url{https://efron.ckirby.su.domains//papers/2005LocalFDR.pdf}}
}

@Article{	  Efron.2005b,
  author =	 {Bradley Efron},
  title =	 {Correlation and Large-Scale Simultaneous Significance Testing},
  journal =	 {Journal of the American Statistical Association},
  volume =	 102,
  number =	 477,
  pages =	 {93-103},
  year =	 2007,
  publisher =	 {Taylor & Francis},
  doi =		 {10.1198/016214506000001211},
  eprint =	 { https://doi.org/10.1198/016214506000001211}
}

@Book{		  Efron.2010a,
  address =	 {Cambridge},
  title =	 {Large-{Scale} {Inference}: {Empirical} {Bayes} {Methods} for {Estimation}, {Testing}, and
                  {Prediction}},
  isbn =	 {978-0-511-76136-2},
  shorttitle =	 {Large-{Scale} {Inference}},
  url =		 {http://ebooks.cambridge.org/ref/id/CBO9780511761362},
  abstract =	 {Modern scientiﬁc technology is providing a new class of large-scale simultaneous inference problems,
                  with hundreds or thousands of hypothesis tests to consider at the same time. Microarrays epitomize
                  this type of technology but similar problems arise in proteomics, time of ﬂight spectroscopy, ﬂow
                  cytometry, FMRI imaging, and massive social science surveys. This paper uses local false discovery
                  rate methods to carry out size and power calculations on large-scale data sets. An empirical Bayes
                  approach allows the fdr analysis to proceed from a minimum of frequentist or Bayesian modeling
                  assumptions. Microarray and simulated data sets are used to illustrate a convenient estimation
                  methodology whose accuracy can be calculated in closed form. A crucial part of the methodology is an
                  fdr assessment of “thinned counts”, what the histogram of test statistics would look like for just the
                  non-null cases.},
  language =	 {en},
  urldate =	 {2019-10-27},
  publisher =	 {Cambridge University Press},
  author =	 {Efron, Bradley},
  year =	 2010,
  doi =		 {10.1017/CBO9780511761362},
  file =	 {Efron - 2010 - Large-Scale Inference Empirical Bayes Methods
                  for.pdf:/home/dun280/local/zotero/storage/9S8CZIX5/Efron - 2010 - Large-Scale Inference Empirical
                  Bayes Methods for.pdf:application/pdf}
}

@Article{	  EfronMicroarraysEmpiricalBayes2008,
  title =	 {Microarrays, {{Empirical Bayes}} and the {{Two}}-{{Groups Model}}},
  author =	 {Efron, Bradley},
  year =	 2008,
  month =	 feb,
  volume =	 23,
  pages =	 {1--22},
  publisher =	 {{Institute of Mathematical Statistics}},
  issn =	 {0883-4237, 2168-8745},
  doi =		 {10.1214/07-STS236},
  abstract =	 {The classic frequentist theory of hypothesis testing developed by Neyman, Pearson and Fisher has a
                  claim to being the twentieth century's most influential piece of applied mathematics. Something new is
                  happening in the twenty-first century: high-throughput devices, such as microarrays, routinely require
                  simultaneous hypothesis tests for thousands of individual cases, not at all what the classical theory
                  had in mind. In these situations empirical Bayes information begins to force itself upon frequentists
                  and Bayesians alike. The two-groups model is a simple Bayesian construction that facilitates empirical
                  Bayes analysis. This article concerns the interplay of Bayesian and frequentist ideas in the
                  two-groups setting, with particular attention focused on Benjamini and Hochberg's False Discovery Rate
                  method. Topics include the choice and meaning of the null hypothesis in large-scale testing
                  situations, power considerations, the limitations of permutation methods, significance testing for
                  groups of cases (such as pathways in microarray studies), correlation effects, multiple confidence
                  intervals and Bayesian competitors to the two-groups model.},
  file =	 {/home/dun280/Dropbox/zotero/Statistical Science/2008/Efron_2008_Microarrays, Empirical Bayes and the
                  Two-Groups Model.pdf},
  journal =	 {Statistical Science},
  keywords =	 {empirical null,false discovery rates,Simultaneous tests},
  language =	 {EN},
  mrnumber =	 {MR2431866},
  number =	 1,
  zmnumber =	 {1327.62047}
}

@Manual{Elzhov.et.al.2022,
  title =	 {minpack.lm: R Interface to the Levenberg-Marquardt Nonlinear Least-Squares Algorithm Found in MINPACK,
                  Plus Support for Bounds},
  author =	 {Timur V. Elzhov and Katharine M. Mullen and Andrej-Nikolai Spiess and Ben Bolker},
  year =	 2022,
  note =	 {R package version 1.2-2},
  url =		 {https://CRAN.R-project.org/package=minpack.lm}
}

@Article{	  Gauran.et.al.2018,
  title =	 {Empirical null estimation using zero-inflated discrete mixture distributions and its application to
                  protein domain data},
  volume =	 74,
  issn =	 {1541-0420},
  url =		 {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12779},
  doi =		 {10.1111/biom.12779},
  abstract =	 {In recent mutation studies, analyses based on protein domain positions are gaining popularity over
                  gene-centric approaches since the latter have limitations in considering the functional context that
                  the position of the mutation provides. This presents a large-scale simultaneous inference problem,
                  with hundreds of hypothesis tests to consider at the same time. This article aims to select
                  significant mutation counts while controlling a given level of Type I error via False Discovery Rate
                  (FDR) procedures.  One main assumption is that the mutation counts follow a zero-inflated model in
                  order to account for the true zeros in the count model and the excess zeros. The class of models
                  considered is the Zero-inflated Generalized Poisson (ZIGP) distribution. Furthermore, we assumed that
                  there exists a cut-off value such that smaller counts than this value are generated from the null
                  distribution. We present several data-dependent methods to determine the cut-off value. We also
                  consider a two-stage procedure based on screening process so that the number of mutations exceeding a
                  certain value should be considered as significant mutations. Simulated and protein domain data sets
                  are used to illustrate this procedure in estimation of the empirical null using a mixture of discrete
                  distributions. Overall, while maintaining control of the FDR, the proposed two-stage testing procedure
                  has superior empirical power.},
  language =	 {en},
  number =	 2,
  urldate =	 {2019-11-08},
  journal =	 {Biometrics},
  author =	 {Gauran, Iris Ivy M. and Park, Junyong and Lim, Johan and Park, DoHwan and Zylstra, John and Peterson,
                  Thomas and Kann, Maricel and Spouge, John L.},
  year =	 2018,
  keywords =	 {local false discovery rate, protein domain, zero inflated generalized Poisson},
  pages =	 {458--471},
  file =	 {Gauran et al_2018_Empirical null estimation using zero-inflated discrete mixture
                  distributions.pdf:/home/dun280/Dropbox/zotero/Gauran et al_2018_Empirical null estimation using
                  zero-inflated discrete mixture distributions.pdf:application/pdf}
}

@Article{Janitza.2016,
  author =	 "Janitza, Silke and Celik, Ender and Boulesteix, Anne-Laure",
  title =	 "A computationally fast variable importance test for random forests for high-dimensional data",
  journal =	 "Advances in Data Analysis and Classification",
  year =	 2016,
  pages =	 "1--31",
  abstract =	 "Random forests are a commonly used tool for classification and for ranking candidate predictors based
                  on the so-called variable importance measures. These measures attribute scores to the variables
                  reflecting their importance. A drawback of variable importance measures is that there is no natural
                  cutoff that can be used to discriminate between important and non-important variables. Several
                  approaches, for example approaches based on hypothesis testing, were developed for addressing this
                  problem. The existing testing approaches require the repeated computation of random forests. While for
                  low-dimensional settings those approaches might be computationally tractable, for high-dimensional
                  settings typically including thousands of candidate predictors, computing time is enormous. In this
                  article a computationally fast heuristic variable importance test is proposed that is appropriate for
                  high-dimensional data where many variables do not carry any information. The testing approach is based
                  on a modified version of the permutation variable importance, which is inspired by cross-validation
                  procedures. The new approach is tested and compared to the approach of Altmann and colleagues using
                  simulation studies, which are based on real data from high-dimensional binary classification
                  settings. The new approach controls the type I error and has at least comparable power at a
                  substantially smaller computation time in the studies. Thus, it might be used as a computationally
                  fast alternative to existing procedures for high-dimensional data settings where many variables do not
                  carry any information. The new approach is implemented in the R package vita.",
  issn =	 "1862-5355",
  doi =		 "10.1007/s11634-016-0270-x",
  url =		 "http://dx.doi.org/10.1007/s11634-016-0270-x"
}

@Misc{Kaggle.2019,
  key =		 {Kaggle 2019},
  author =	 {Kaggle},
  year =	 2019,
  title =	 {{Historical Data Science Trends on Kaggle}},
  note =	 {Accessed: 2019-04-20 }
}

@Article{Kursa.and.Rudnicki.2010,
  title =	 {Feature Selection with the {Boruta} Package},
  author =	 {Miron B. Kursa and Witold R. Rudnicki},
  journal =	 {Journal of Statistical Software},
  year =	 2010,
  volume =	 36,
  number =	 11,
  pages =	 {1--13},
  url =		 {https://www.jstatsoft.org/v36/i11/},
}

@Article{LaPointe.et.al.2012,
  author =	 {Lawrence C. LaPointe and Susanne K. Pedersen and Robert Dunne and Glenn S. Brown and Letitia Pimlott
                  and Snigdha Gaur and Aidan McEvoy and Melissa Thomas and David Wattchow and Peter L.  Molloy and
                  Graeme P. Young},
  title =	 {Discovery and Validation of Molecular Biomarkers for Colorectal Adenomas and Cancer with Application
                  to Blood Testing},
  journal =	 {PLoS ONE},
  year =	 2012,
  volume =	 7,
  number =	 1,
  doi =		 {10.1371/journal.pone.0029059}
}

@Article{	  Lemhadri.et.al.2021,
  title =	 {{{LassoNet}}: {{A}} Neural Network with Feature Sparsity},
  author =	 {Lemhadri, Ismael and Ruan, Feng and Abraham, Louis and Tibshirani, Robert},
  year =	 2021,
  volume =	 22,
  pages =	 {1--29},
  journal =	 {Journal of Machine Learning Research},
  number =	 127
}

@Misc{Lundberg.et.al.2022,
  author =	 {Mischa Lundberg and Letitia M.F. Sng and Piotr Szul and Rob Dunne and Arash Bayat and Samantha Burnham
                  and Gabriel Cuellar-Partida and Denis C. Bauer and Natalie A. Twine},
  title =	 {VariantSpark, a cloud-based random forest GWAS platform, identifies novel loci and epistasis in
                  Alzheimer's disease},
  howpublished = {Submitted to Nature Aging},
  year =	 2022
}

@Manual{Maechler.2021,
  title =	 {diptest: Hartigan's Dip Test Statistic for Unimodality - Corrected},
  author =	 {Martin Maechler},
  year =	 2021,
  note =	 {R package version 0.76-0},
  url =		 {https://CRAN.R-project.org/package=diptest},
}

@Article{	  Michaelson.et.al.2010,
  title =	 {Data-driven assessment of {eQTL} mapping methods},
  volume =	 11,
  issn =	 {1471-2164},
  url =		 {https://doi.org/10.1186/1471-2164-11-502},
  doi =		 {10.1186/1471-2164-11-502},
  abstract =	 {The analysis of expression quantitative trait loci (eQTL) is a potentially powerful way to detect
                  transcriptional regulatory relationships at the genomic scale. However, eQTL data sets often go
                  underexploited because legacy QTL methods are used to map the relationship between the expression
                  trait and genotype. Often these methods are inappropriate for complex traits such as gene expression,
                  particularly in the case of epistasis.},
  urldate =	 {2018-03-16},
  journal =	 {BMC Genomics},
  author =	 {Michaelson, Jacob J. and Alberts, Rudi and Schughart, Klaus and Beyer, Andreas},
  month =	 sep,
  year =	 2010,
  keywords =	 {Composite Interval Mapping, Importance Measure, Lasso, Quantitative Trait Locus, Random Forest},
  pages =	 502,
  annote =	 {Pages 502 in PDF}
}

@Article{	  Nembrini.et.al.2018,
  title =	 {The revival of the {Gini} importance?},
  url =		 {https://academic.oup.com/bioinformatics/advance-article/doi/10.1093/bioinformatics/bty373/4994791},
  doi =		 {10.1093/bioinformatics/bty373},
  abstract =	 {AbstractMotivation. Random forests are fast, flexible and represent a robust approach to analyze high
                  dimensional data. A key advantage over alternative machin},
  language =	 {en},
  urldate =	 {2018-07-05},
  journal =	 {Bioinformatics},
  year =	 2018,
  author =	 {Nembrini, Stefano and K\"onig, Inke R. and Wright, Marvin N. and Valencia, Alfonso},
  file =	 {Full Text PDF:/home/dun280/local/zotero/storage/2HFUKPHD/Nembrini et al. - The revival of the Gini
                  importance.pdf:application/pdf;Snapshot:/home/dun280/local/zotero/storage/GEXCCUDR/4994791.html:text/html}
}

@Unpublished{	  OBrien.et.al.2018,
  title =	 {CursedForest- A Random Forest Implementation for "Big" and "Wide" Data},
  author =	 {Aidan O'Brien and Piotr Szul and Robert Dunne and Paul Leo and Emma L. Duncan and Natalie A. Twine and
                  Stephanie Li and James Doecke and Nick Ellis and Denis C. Bauer},
  year =	 2018,
  note =	 {submitted}
}

@Article{	  Price.et.al.2006,
  title =	 {Principal components analysis corrects for stratification in genome-wide association studies},
  volume =	 38,
  copyright =	 {2006 Nature Publishing Group},
  issn =	 {1546-1718},
  url =		 {https://www.nature.com/articles/ng1847},
  doi =		 {10.1038/ng1847},
  abstract =	 {Population stratification—allele frequency differences between cases and controls due to systematic
                  ancestry differences—can cause spurious associations in disease studies. We describe a method that
                  enables explicit detection and correction of population stratification on a genome-wide scale. Our
                  method uses principal components analysis to explicitly model ancestry differences between cases and
                  controls. The resulting correction is specific to a candidate marker's variation in frequency across
                  ancestral populations, minimizing spurious associations while maximizing power to detect true
                  associations. Our simple, efficient approach can easily be applied to disease studies with hundreds of
                  thousands of markers.},
  language =	 {en},
  number =	 8,
  urldate =	 {2018-07-02},
  journal =	 {Nature Genetics},
  author =	 {Price, Alkes L. and Patterson, Nick J. and Plenge, Robert M. and Weinblatt, Michael E. and Shadick,
                  Nancy A. and Reich, David},
  month =	 aug,
  year =	 2006,
  pages =	 {904--909},
  file =	 {Full Text PDF:/home/dun280/local/zotero/storage/XYF7W76J/Price et al.  - 2006 - Principal components
                  analysis corrects for
                  stratif.pdf:application/pdf;Snapshot:/home/dun280/local/zotero/storage/K9SGA4ZN/ng1847.html:text/html}
}

@Misc{		  Segal.2004,
  title =	 {Machine Learning Benchmarks and Random Forest Regression},
  author =	 {Mark R. Segal},
  month =	 apr,
  year =	 2004,
  publisher =	 {Center for Bioinformatics and Molecular Biostatistics},
  url =		 {http://escholarship.org/uc/item/35x3v9t4}
}

@Article{Spira.et.al.2004,
  author =	 {Spira, A. E. and Beane, J. and Pinto-Plata, V.  and Kadar, A. and Liu, G. and Shah, V. and Celli,
                  B. and Brody, J. S.},
  title =	 {Gene Expression Profiling of Human Lung Tissue from Smokers with Severe Emphysema},
  journal =	 {Am J Respir Cell Mol Biol.},
  year =	 2004
}

@Article{	  Strobl.et.al.2007,
  title =	 {Bias in Random Forest Variable Importance Measures: {I}llustrations, Sources and a Solution},
  author =	 {Carolin Strobl and Anne-Laure Boulesteix and Achim Zeileis and Torsten Hothorn},
  journal =	 {BMC Bioinformatics},
  year =	 2007,
  note =	 {highly accessed},
  pages =	 25,
  volume =	 8,
  code =	 {http://CRAN.R-project.org/package=party},
  doi =		 {10.1186/1471-2105-8-25},
  gs =		 7766317648828457249,
  url =		 {http://www.biomedcentral.com/1471-2105/8/25/abstract}
}

@Article{	  Strobl.et.al.2008,
  author =	 "Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim",
  title =	 "Conditional variable importance for random forests",
  journal =	 "BMC Bioinformatics",
  year =	 2008,
  month =	 "Jul",
  day =		 11,
  volume =	 9,
  number =	 1,
  pages =	 307,
  abstract =	 "Random forests are becoming increasingly popular in many scientific fields because they can cope with
                  ``small n large p'' problems, complex interactions and even highly correlated predictor
                  variables. Their variable importance measures have recently been suggested as screening tools for,
                  e.g., gene expression studies. However, these variable importance measures show a bias towards
                  correlated predictor variables.",
  issn =	 "1471-2105",
  doi =		 "10.1186/1471-2105-9-307",
  url =		 "https://doi.org/10.1186/1471-2105-9-307"
}

@ARTICLE{Welling.et.al.2016,
  author =	 {{Welling}, S.~H. and {Refsgaard}, H.~H.~F. and {Brockhoff}, P.~B. and {Clemmensen}, L.~H.},
  title =	 "{Forest Floor Visualizations of Random Forests}",
  journal =	 {ArXiv e-prints},
  archivePrefix ="arXiv",
  eprint =	 {1605.09196},
  primaryClass = "stat.ML",
  keywords =	 {Statistics - Machine Learning, Computer Science - Learning},
  year =	 2016,
  month =	 may,
  adsurl =	 {http://adsabs.harvard.edu/abs/2016arXiv160509196W},
  adsnote =	 {Provided by the SAO/NASA Astrophysics Data System},
  url =		 "https://arxiv.org/pdf/1605.09196.pdf"
}

@Article{	  Witten.and.Tibshirani.2008,
  title =	 {Testing significance of features by lassoed principal components},
  volume =	 2,
  issn =	 {1932-6157, 1941-7330},
  url =		 {https://projecteuclid.org/euclid.aoas/1223908049},
  doi =		 {10.1214/08-AOAS182},
  abstract =	 {We consider the problem of testing the significance of features in high-dimensional settings. In
                  particular, we test for differentially-expressed genes in a microarray experiment. We wish to identify
                  genes that are associated with some type of outcome, such as survival time or cancer type. We propose
                  a new procedure, called Lassoed Principal Components (LPC), that builds upon existing methods and can
                  provide a sizable improvement. For instance, in the case of two-class data, a standard (albeit simple)
                  approach might be to compute a two-sample t-statistic for each gene. The LPC method involves
                  projecting these conventional gene scores onto the eigenvectors of the gene expression data covariance
                  matrix and then applying an L1 penalty in order to de-noise the resulting projections. We present a
                  theoretical framework under which LPC is the logical choice for identifying significant genes, and we
                  show that LPC can provide a marked reduction in false discovery rates over the conventional methods on
                  both real and simulated data.  Moreover, this flexible procedure can be applied to a variety of types
                  of data and can be used to improve many existing methods for the identification of significant
                  features.},
  language =	 {EN},
  number =	 3,
  urldate =	 {2017-12-27},
  journal =	 {Ann. Appl. Stat.},
  author =	 {Witten, Daniela M. and Tibshirani, Robert},
  month =	 sep,
  year =	 2008,
  mrnumber =	 {MR2516801},
  zmnumber =	 {1149.62092},
  keywords =	 {feature selection, gene expression, Microarray, multiple testing},
  pages =	 {986--1012}
}

@Article{Wright.and.Ziegler.2017,
  title =	 {{ranger}: A Fast Implementation of Random Forests for High Dimensional Data in {C++} and {R}},
  author =	 {Marvin N. Wright and Andreas Ziegler},
  journal =	 {Journal of Statistical Software},
  year =	 2017,
  volume =	 77,
  number =	 1,
  pages =	 {1--17},
  doi =		 {10.18637/jss.v077.i01},
}

@Article{alipanahiLargescaleMachinelearningbasedPhenotyping2021,
  title =	 {Large-Scale Machine-Learning-Based Phenotyping Significantly Improves Genomic Discovery for Optic
                  Nerve Head Morphology},
  author =	 {Alipanahi, Babak and Hormozdiari, Farhad and Behsaz, Babak and Cosentino, Justin and McCaw, Zachary
                  R. and Schorsch, Emanuel and Sculley, D. and Dorfman, Elizabeth H. and Foster, Paul J. and Peng, Lily
                  H. and Phene, Sonia and Hammel, Naama and Carroll, Andrew and Khawaja, Anthony P.  and McLean, Cory
                  Y.},
  year =	 2021,
  month =	 jul,
  volume =	 108,
  pages =	 {1217--1230},
  publisher =	 {{Elsevier}},
  issn =	 {0002-9297, 1537-6605},
  doi =		 {10.1016/j.ajhg.2021.05.004},
  file =	 {/home/dun280/Dropbox/zotero/Alipanahi et al/Alipanahi et al_2021_Large-scale machine-learning-based
                  phenotyping significantly improves genomic.pdf},
  journal =	 {The American Journal of Human Genetics},
  keywords =	 {glaucoma,GWAS,machine learning,phenotyping},
  language =	 {English},
  number =	 7,
  pmid =	 34077760
}

@Article{arikTabNetAttentiveInterpretable2020,
  title =	 {{{TabNet}}: Attentive {{Interpretable Tabular Learning}}},
  shorttitle =	 {{{TabNet}}},
  author =	 {Arik, Sercan O. and Pfister, Tomas},
  year =	 2020,
  month =	 dec,
  journal =	 {arXiv:1908.07442 [cs, stat]},
  eprint =	 {1908.07442},
  eprinttype =	 {arxiv},
  primaryclass = {cs, stat},
  abstract =	 {We propose a novel high-performance and interpretable canonical deep tabular data learning
                  architecture, TabNet.  TabNet uses sequential attention to choose which features to reason from at
                  each decision step, enabling interpretability and more efficient learning as the learning capacity is
                  used for the most salient features. We demonstrate that TabNet outperforms other neural network and
                  decision tree variants on a wide range of non-performance-saturated tabular datasets and yields
                  interpretable feature attributions plus insights into the global model behavior. Finally, for the
                  first time to our knowledge, we demonstrate self-supervised learning for tabular data, significantly
                  improving performance with unsupervised representation learning when unlabeled data is abundant.},
  archiveprefix ={arXiv},
  keywords =	 {Computer Science - Machine Learning,Statistics - Machine Learning},
  file =	 {/home/dun280/Dropbox/zotero/Arik_Pfister/Arik_Pfister_2020_TabNet.pdf}
}

@article{autonGlobalReferenceHuman2015,
  title =	 {A Global Reference for Human Genetic Variation},
  author =	 {Auton, Adam and Abecasis, Gon{\c c}alo R. and Altshuler, David M. and {298 other authors}},
  year =	 2015,
  month =	 oct,
  journal =	 {Nature},
  volume =	 526,
  number =	 7571,
  pages =	 {68--74},
  publisher =	 {{Nature Publishing Group}},
  issn =	 {1476-4687},
  doi =		 {10.1038/nature15393},
  abstract =	 {The 1000 Genomes Project set out to provide a comprehensive description of common human genetic
                  variation by applying whole-genome sequencing to a diverse set of individuals from multiple
                  populations. Here we report completion of the project, having reconstructed the genomes of 2,504
                  individuals from 26 populations using a combination of low-coverage whole-genome sequencing, deep
                  exome sequencing, and dense microarray genotyping. We characterized a broad spectrum of genetic
                  variation, in total over 88 million variants (84.7 million single nucleotide polymorphisms (SNPs), 3.6
                  million short insertions/deletions (indels), and 60,000 structural variants), all phased onto
                  high-quality haplotypes. This resource includes {$>$}99\% of SNP variants with a frequency of {$>$}1\%
                  for a variety of ancestries. We describe the distribution of genetic variation across the global
                  sample, and discuss the implications for common disease studies.},
  copyright =	 {2015 The Author(s)},
  langid =	 {english},
  keywords =	 {Genetic variation,Genomics},
  file =	 {/home/dun280/Dropbox/zotero/Auton et al/Auton et al_2015_A global reference for human genetic
                  variation.pdf},
  nynote =	 {the 1000 Genomes reference}
}

@article{autonGlobalReferenceHuman2015-full-citation,
  title =	 {A Global Reference for Human Genetic Variation},
  author =	 {Auton, Adam and Abecasis, Gon{\c c}alo R. and Altshuler, David M. and Durbin, Richard M. and Abecasis,
                  Gon{\c c}alo R. and Bentley, David R. and Chakravarti, Aravinda and Clark, Andrew G. and Donnelly,
                  Peter and Eichler, Evan E. and Flicek, Paul and Gabriel, Stacey B. and Gibbs, Richard A. and Green,
                  Eric D. and Hurles, Matthew E. and Knoppers, Bartha M. and Korbel, Jan O. and Lander, Eric S. and Lee,
                  Charles and Lehrach, Hans and Mardis, Elaine R. and Marth, Gabor T. and McVean, Gil A. and Nickerson,
                  Deborah A. and Schmidt, Jeanette P. and Sherry, Stephen T. and Wang, Jun and Wilson, Richard K. and
                  Gibbs, Richard A. and Boerwinkle, Eric and Doddapaneni, Harsha and Han, Yi and Korchina, Viktoriya and
                  Kovar, Christie and Lee, Sandra and Muzny, Donna and Reid, Jeffrey G. and Zhu, Yiming and Wang, Jun
                  and Chang, Yuqi and Feng, Qiang and Fang, Xiaodong and Guo, Xiaosen and Jian, Min and Jiang, Hui and
                  Jin, Xin and Lan, Tianming and Li, Guoqing and Li, Jingxiang and Li, Yingrui and Liu, Shengmao and
                  Liu, Xiao and Lu, Yao and Ma, Xuedi and Tang, Meifang and Wang, Bo and Wang, Guangbiao and Wu,
                  Honglong and Wu, Renhua and Xu, Xun and Yin, Ye and Zhang, Dandan and Zhang, Wenwei and Zhao, Jiao and
                  Zhao, Meiru and Zheng, Xiaole and Lander, Eric S. and Altshuler, David M. and Gabriel, Stacey B. and
                  Gupta, Namrata and Gharani, Neda and Toji, Lorraine H. and Gerry, Norman P. and Resch, Alissa M. and
                  Flicek, Paul and Barker, Jonathan and Clarke, Laura and Gil, Laurent and Hunt, Sarah E. and Kelman,
                  Gavin and Kulesha, Eugene and Leinonen, Rasko and McLaren, William M. and Radhakrishnan, Rajesh and
                  Roa, Asier and Smirnov, Dmitriy and Smith, Richard E. and Streeter, Ian and Thormann, Anja and Toneva,
                  Iliana and Vaughan, Brendan and {Zheng-Bradley}, Xiangqun and Bentley, David R. and Grocock, Russell
                  and Humphray, Sean and James, Terena and Kingsbury, Zoya and Lehrach, Hans and Sudbrak, Ralf and
                  Albrecht, Marcus W. and Amstislavskiy, Vyacheslav S. and Borodina, Tatiana A. and Lienhard, Matthias
                  and Mertes, Florian and Sultan, Marc and Timmermann, Bernd and Yaspo, Marie-Laure and Mardis, Elaine
                  R. and Wilson, Richard K. and Fulton, Lucinda and Fulton, Robert and Sherry, Stephen T. and Ananiev,
                  Victor and Belaia, Zinaida and Beloslyudtsev, Dimitriy and Bouk, Nathan and Chen, Chao and Church,
                  Deanna and Cohen, Robert and Cook, Charles and Garner, John and Hefferon, Timothy and Kimelman,
                  Mikhail and Liu, Chunlei and Lopez, John and Meric, Peter and O'Sullivan, Chris and Ostapchuk, Yuri
                  and Phan, Lon and Ponomarov, Sergiy and Schneider, Valerie and Shekhtman, Eugene and Sirotkin, Karl
                  and Slotta, Douglas and Zhang, Hua and McVean, Gil A. and Durbin, Richard M. and Balasubramaniam,
                  Senduran and Burton, John and Danecek, Petr and Keane, Thomas M. and {Kolb-Kokocinski}, Anja and
                  McCarthy, Shane and Stalker, James and Quail, Michael and Schmidt, Jeanette P. and Davies, Christopher
                  J. and Gollub, Jeremy and Webster, Teresa and Wong, Brant and Zhan, Yiping and Auton, Adam and
                  Campbell, Christopher L. and Kong, Yu and Marcketta, Anthony and Gibbs, Richard A. and Yu, Fuli and
                  Antunes, Lilian and Bainbridge, Matthew and Muzny, Donna and Sabo, Aniko and Huang, Zhuoyi and Wang,
                  Jun and Coin, Lachlan J. M. and Fang, Lin and Guo, Xiaosen and Jin, Xin and Li, Guoqing and Li, Qibin
                  and Li, Yingrui and Li, Zhenyu and Lin, Haoxiang and Liu, Binghang and Luo, Ruibang and Shao, Haojing
                  and Xie, Yinlong and Ye, Chen and Yu, Chang and Zhang, Fan and Zheng, Hancheng and Zhu, Hongmei and
                  Alkan, Can and Dal, Elif and Kahveci, Fatma and Marth, Gabor T. and Garrison, Erik P. and Kural, Deniz
                  and Lee, Wan-Ping and Fung Leong, Wen and Stromberg, Michael and Ward, Alistair N. and Wu, Jiantao and
                  Zhang, Mengyao and Daly, Mark J. and DePristo, Mark A. and Handsaker, Robert E. and Altshuler, David
                  M. and Banks, Eric and Bhatia, Gaurav and {del Angel}, Guillermo and Gabriel, Stacey B. and Genovese,
                  Giulio and Gupta, Namrata and Li, Heng and Kashin, Seva and Lander, Eric S. and McCarroll, Steven
                  A. and Nemesh, James C. and Poplin, Ryan E. and Yoon, Seungtai C. and Lihm, Jayon and Makarov,
                  Vladimir and Clark, Andrew G. and Gottipati, Srikanth and Keinan, Alon and {Rodriguez-Flores}, Juan
                  L. and Korbel, Jan O. and Rausch, Tobias and Fritz, Markus H. and St{\"u}tz, Adrian M. and Flicek,
                  Paul and Beal, Kathryn and Clarke, Laura and Datta, Avik and Herrero, Javier and McLaren, William
                  M. and Ritchie, Graham R. S. and Smith, Richard E. and Zerbino, Daniel and {Zheng-Bradley}, Xiangqun
                  and Sabeti, Pardis C. and Shlyakhter, Ilya and Schaffner, Stephen F. and Vitti, Joseph and Cooper,
                  David N. and Ball, Edward V. and Stenson, Peter D. and Bentley, David R. and Barnes, Bret and Bauer,
                  Markus and Keira Cheetham, R. and Cox, Anthony and Eberle, Michael and Humphray, Sean and Kahn, Scott
                  and Murray, Lisa and Peden, John and Shaw, Richard and Kenny, Eimear E. and Batzer, Mark A. and
                  Konkel, Miriam K. and Walker, Jerilyn A. and MacArthur, Daniel G. and Lek, Monkol and Sudbrak, Ralf
                  and Amstislavskiy, Vyacheslav S. and Herwig, Ralf and Mardis, Elaine R. and Ding, Li and Koboldt,
                  Daniel C. and Larson, David and Ye, Kai and Gravel, Simon and {The 1000 Genomes Project Consortium}
                  and {Corresponding authors} and {Steering committee} and {Production group} and {Baylor College of
                  Medicine} and {BGI-Shenzhen} and {Broad Institute of MIT and Harvard} and {Coriell Institute for
                  Medical Research} and European Molecular Biology Laboratory, European Bioinformatics Institute and
                  {Illumina} and {Max Planck Institute for Molecular Genetics} and {McDonnell Genome Institute at
                  Washington University} and {US National Institutes of Health} and {University of Oxford} and {Wellcome
                  Trust Sanger Institute} and {Analysis group} and {Affymetrix} and {Albert Einstein College of
                  Medicine} and {Bilkent University} and {Boston College} and {Cold Spring Harbor Laboratory} and
                  {Cornell University} and {European Molecular Biology Laboratory} and {Harvard University} and {Human
                  Gene Mutation Database} and {Icahn School of Medicine at Mount Sinai} and {Louisiana State University}
                  and {Massachusetts General Hospital} and {McGill University} and National Eye Institute, NIH},
  year =	 2015,
  month =	 oct,
  journal =	 {Nature},
  volume =	 526,
  number =	 7571,
  pages =	 {68--74},
  publisher =	 {{Nature Publishing Group}},
  issn =	 {1476-4687},
  doi =		 {10.1038/nature15393},
  abstract =	 {The 1000 Genomes Project set out to provide a comprehensive description of common human genetic
                  variation by applying whole-genome sequencing to a diverse set of individuals from multiple
                  populations. Here we report completion of the project, having reconstructed the genomes of 2,504
                  individuals from 26 populations using a combination of low-coverage whole-genome sequencing, deep
                  exome sequencing, and dense microarray genotyping. We characterized a broad spectrum of genetic
                  variation, in total over 88 million variants (84.7 million single nucleotide polymorphisms (SNPs), 3.6
                  million short insertions/deletions (indels), and 60,000 structural variants), all phased onto
                  high-quality haplotypes. This resource includes {$>$}99\% of SNP variants with a frequency of {$>$}1\%
                  for a variety of ancestries. We describe the distribution of genetic variation across the global
                  sample, and discuss the implications for common disease studies.},
  copyright =	 {2015 The Author(s)},
  langid =	 {english},
  keywords =	 {Genetic variation,Genomics},
  file =	 {/home/dun280/Dropbox/zotero/Auton et al/Auton et al_2015_A global reference for human genetic
                  variation.pdf},
  nynote =	 {the 1000 Genomes reference}
}

@article{bayatFastAccurateExhaustive2021,
  title =	 {Fast and Accurate Exhaustive Higher-Order Epistasis Search with {{BitEpi}}},
  author =	 {Bayat, Arash and Hosking, Brendan and Jain, Yatish and Hosking, Cameron and Kodikara, Milindi and
                  Reti, Daniel and Twine, Natalie A. and Bauer, Denis C.},
  date =	 {2021-08-05},
  year =	 2021,
  journaltitle = {Scientific Reports},
  journal =	 {Scientific Reports},
  shortjournal = {Sci Rep},
  volume =	 11,
  number =	 1,
  eprint =	 34354094,
  eprinttype =	 {pmid},
  pages =	 15923,
  issn =	 {2045-2322},
  doi =		 {10.1038/s41598-021-94959-y},
  abstract =	 {Complex genetic diseases may be modulated by a large number of epistatic interactions affecting a
                  polygenic phenotype. Identifying these interactions is difficult due to computational complexity,
                  especially in the case of higher-order interactions where more than two genomic variants are
                  involved. In this paper, we present BitEpi, a fast and accurate method to test all possible
                  combinations of up to four bi-allelic variants (i.e. Single Nucleotide Variant or SNV for
                  short). BitEpi introduces a novel bitwise algorithm that is 1.7 and 56 times faster for 3-SNV and
                  4-SNV search, than established software. The novel entropy statistic used in BitEpi is 44\% more
                  accurate to identify interactive SNVs, incorporating a p-value-based significance testing. We
                  demonstrate BitEpi on real world data of 4900 samples and 87,000 SNPs. We also present EpiExplorer to
                  visualize the potentially large number of individual and interacting SNVs in an interactive Cytoscape
                  graph. EpiExplorer uses various visual elements to facilitate the discovery of true biological events
                  in a complex polygenic environment.},
  langid =	 {english},
  pmcid =	 {PMC8342486},
  file =	 {/home/dun280/Dropbox/zotero/Scientific Reports/2021/Bayat et al_2021_Fast and accurate exhaustive
                  higher-order epistasis search with BitEpi.pdf}
}

@Article{	  clarkeInternationalGenomeSample2017,
  title =	 {The International {{Genome}} Sample Resource ({{IGSR}}): {{A}} Worldwide Collection of Genome
                  Variation Incorporating the 1000 {{Genomes Project}} Data},
  shorttitle =	 {The International {{Genome}} Sample Resource ({{IGSR}})},
  author =	 {Clarke, Laura and Fairley, Susan and {Zheng-Bradley}, Xiangqun and Streeter, Ian and Perry, Emily and
                  Lowy, Ernesto and Tass{\'e}, Anne-Marie and Flicek, Paul},
  year =	 2017,
  month =	 jan,
  journal =	 {Nucleic Acids Research},
  volume =	 45,
  number =	 {D1},
  pages =	 {D854-D859},
  issn =	 {0305-1048},
  doi =		 {10.1093/nar/gkw829},
  abstract =	 {The International Genome Sample Resource (IGSR; http://www.internationalgenome.org) expands in data
                  type and population diversity the resources from the 1000 Genomes Project. IGSR represents the largest
                  open collection of human variation data and provides easy access to these resources. IGSR was
                  established in 2015 to maintain and extend the 1000 Genomes Project data, which has been widely used
                  as a reference set of human variation and by researchers developing analysis methods. IGSR has mapped
                  all of the 1000 Genomes sequence to the newest human reference (GRCh38), and will release updated
                  variant calls to ensure maximal usefulness of the existing data. IGSR is collecting new structural
                  variation data on the 1000 Genomes samples from long read sequencing and other technologies, and will
                  collect relevant functional data into a single comprehensive resource. IGSR is extending coverage with
                  new populations sequenced by collaborating groups. Here, we present the new data and analysis that
                  IGSR has made available. We have also introduced a new data portal that increases discoverability of
                  our data\textemdash previously only browseable through our FTP site\textemdash by focusing on
                  particular samples, populations or data sets of interest.},
  file =	 {/home/dun280/Dropbox/zotero/Clarke et al/Clarke et al_2017_The international Genome sample resource
                  (IGSR).pdf},
  mynote =	 {the UK Biobank reference}
}

@misc{ enwiki:1081722972,
  author =	 "{Wikipedia contributors}",
  title =	 "Genome-wide association study --- {Wikipedia}{,} The Free Encyclopedia",
  year =	 2022,
  howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Genome-wide_association_study&oldid=1081722972}",
  note =	 "[Online; accessed 10-April-2022]"
}

@article{friedmanRegularizationPathsGeneralized2010,
  title =	 {Regularization {{Paths}} for {{Generalized Linear Models}} via {{Coordinate Descent}}},
  author =	 {Friedman, Jerome H. and Hastie, Trevor and Tibshirani, Rob},
  date =	 {2010-02-02},
  year =	 2010,
  journaltitle = {Journal of Statistical Software},
  journal =	 {Journal of Statistical Software},
  volume =	 33,
  number =	 1,
  pages =	 {1--22},
  issn =	 {1548-7660},
  doi =		 {10.18637/jss.v033.i01},
  url =		 {https://www.jstatsoft.org/index.php/jss/article/view/v033i01},
  urldate =	 {2020-09-27},
  issue =	 1,
  langid =	 {english},
  file =	 {/home/dun280/Dropbox/zotero/Journal of Statistical Software/2010/Friedman et al_2010_Regularization
                  Paths for Generalized Linear Models via Coordinate Descent.pdf}
}

@article{georgeEagleBetterGenomewide2021,
  title =	 {Eagle for Better Genome-Wide Association Mapping},
  author =	 {George, Andrew W and Verbyla, Arunas and Bowden, Joshua},
  year =	 2021,
  month =	 sep,
  journal =	 {G3 Genes|Genomes|Genetics},
  volume =	 11,
  number =	 9,
  pages =	 {jkab204},
  issn =	 {2160-1836},
  doi =		 {10.1093/g3journal/jkab204},
  abstract =	 {Eagle is an R package for multi-locus association mapping on a genome-wide scale. It is unlike other
                  multi-locus packages in that it is easy to use for R users and non-users alike. It has two modes of
                  use, command line and graphical user interface. Eagle is fully documented and has its own supporting
                  website, http://eagle.r-forge.r-project.org/index.html. Eagle is a significant improvement over the
                  method-of-choice, single-locus association mapping. It has greater power to detect SNP-trait
                  associations. It is based on model selection, linear mixed models, and a clever idea on how random
                  effects can be used to identify SNP-trait associations. Through an example with real mouse data, we
                  demonstrate Eagle's ability to bring clarity and increased insight to single-locus
                  findings. Initially, we see Eagle complementing single-locus analyses. However, over time, we hope the
                  community will make, increasingly, multi-locus association mapping their method-of-choice for the
                  analysis of genome-wide association study data.},
  file =	 {/home/dun280/Dropbox/zotero/George et al/George et al_2021_Eagle for better genome-wide association
                  mapping.pdf}
}

@Article{	  grompingVariableImportanceAssessment2009,
  title =	 {Variable {{Importance Assessment}} in {{Regression}}: {{Linear Regression}} versus {{Random Forest}}},
  shorttitle =	 {Variable {{Importance Assessment}} in {{Regression}}},
  author =	 {Gr{\"o}mping, Ulrike},
  year =	 2009,
  month =	 nov,
  volume =	 63,
  pages =	 {308--319},
  issn =	 {0003-1305},
  doi =		 {10.1198/tast.2009.08199},
  abstract =	 {Relative importance of regressor variables is an old topic that still awaits a satisfactory
                  solution. When interest is in attributing importance in linear regression, averaging over orderings
                  methods for decomposing R2 are among the state-of-the-art methods, although the mechanism behind their
                  behavior is not (yet) completely understood. Random forests\textemdash a machine-learning tool for
                  classification and regression proposed a few years ago\textemdash have an inherent procedure of
                  producing variable importances. This article compares the two approaches (linear model on the one hand
                  and two versions of random forests on the other hand) and finds both striking similarities and
                  differences, some of which can be explained whereas others remain a challenge. The investigation
                  improves understanding of the nature of variable importance in random forests. This article has
                  supplementary material online.},
  file =	 {/home/dun280/local/share/zotero/storage/UJU72G8Q/Grömping - 2009 - Variable Importance Assessment in
                  Regression Line.pdf;/home/dun280/local/share/zotero/storage/FYQ6XTBU/tast.2009.html},
  journal =	 {The American Statistician},
  keywords =	 {Linear model,Random forest,Variable importance},
  number =	 4
}

@Article{	  hothorn_unbiased_2006,
  title =	 {Unbiased {Recursive} {Partitioning}: {A} {Conditional} {Inference} {Framework}},
  volume =	 15,
  issn =	 {1061-8600},
  shorttitle =	 {Unbiased {Recursive} {Partitioning}},
  url =		 {https://doi.org/10.1198/106186006X133933},
  doi =		 {10.1198/106186006X133933},
  abstract =	 {Recursive binary partitioning is a popular tool for regression analysis. Two fundamental problems of
                  exhaustive search procedures usually applied to fit such models have been known for a long time:
                  overfitting and a selection bias towards covariates with many possible splits or missing values. While
                  pruning procedures are able to solve the overfitting problem, the variable selection bias still
                  seriously affects the interpretability of tree-structured regression models. For some special cases
                  unbiased procedures have been suggested, however lacking a common theoretical foundation. We propose a
                  unified framework for recursive partitioning which embeds tree-structured regression models into a
                  well defined theory of conditional inference procedures. Stopping criteria based on multiple test
                  procedures are implemented and it is shown that the predictive performance of the resulting trees is
                  as good as the performance of established exhaustive search procedures. It turns out that the
                  partitions and therefore the models induced by both approaches are structurally different, confirming
                  the need for an unbiased variable selection. Moreover, it is shown that the prediction accuracy of
                  trees with early stopping is equivalent to the prediction accuracy of pruned trees with unbiased
                  variable selection. The methodology presented here is applicable to all kinds of regression problems,
                  including nominal, ordinal, numeric, censored as well as multivariate response variables and arbitrary
                  measurement scales of the covariates. Data from studies on glaucoma classification, node positive
                  breast cancer survival and mammography experience are re-analyzed.},
  number =	 3,
  urldate =	 {2018-08-06},
  journal =	 {Journal of Computational and Graphical Statistics},
  author =	 {Hothorn, Torsten and Hornik, Kurt and Zeileis, Achim},
  month =	 sep,
  year =	 2006,
  keywords =	 {Multiple testing, Multivariate regression trees, Ordinal regression trees, Permutation tests, Variable
                  selection},
  pages =	 {651--674},
  file =	 {Full Text PDF:/home/dun280/local/zotero/storage/7IEG2UQC/Hothorn et al. - 2006 - Unbiased Recursive
                  Partitioning A Conditional
                  Inf.pdf:application/pdf;Snapshot:/home/dun280/local/zotero/storage/NKHAD7Y6/106186006X133933.html:text/html}
}

@Article{	  huynh-thuStatisticalInterpretationMachine2012,
  title =	 {Statistical Interpretation of Machine Learning-Based Feature Importance Scores for Biomarker
                  Discovery},
  author =	 {{Huynh-Thu}, V{\^a}n Anh and Saeys, Yvan and Wehenkel, Louis and Geurts, Pierre},
  year =	 2012,
  month =	 apr,
  volume =	 28,
  issn =	 {1367-4803},
  doi =		 {10.1093/bioinformatics/bts238},
  file =	 {/home/dun280/local/share/zotero/storage/XNS8XZEB/Huynh-Thu et al. - 2012 - Statistical interpretation
                  of machine learning-bas.pdf;/home/dun280/local/share/zotero/storage/YVGT6VU7/126103.html},
  journal =	 {Bioinformatics},
  language =	 {en},
  number =	 13
}

@article{janitzaOverestimationRandomForest2018,
  title =	 {On the Overestimation of Random Forest's out-of-Bag Error},
  author =	 {Janitza, Silke and Hornung, Roman},
  year =	 2018,
  month =	 aug,
  journal =	 {PLOS ONE},
  volume =	 13,
  number =	 8,
  pages =	 {e0201904},
  publisher =	 {{Public Library of Science}},
  issn =	 {1932-6203},
  doi =		 {10.1371/journal.pone.0201904},
  abstract =	 {The ensemble method random forests has become a popular classification tool in bioinformatics and
                  related fields. The out-of-bag error is an error estimation technique often used to evaluate the
                  accuracy of a random forest and to select appropriate values for tuning parameters, such as the number
                  of candidate predictors that are randomly drawn for a split, referred to as mtry. However, for binary
                  classification problems with metric predictors it has been shown that the out-of-bag error can
                  overestimate the true prediction error depending on the choices of random forests parameters. Based on
                  simulated and real data this paper aims to identify settings for which this overestimation is
                  likely. It is, moreover, questionable whether the out-of-bag error can be used in classification tasks
                  for selecting tuning parameters like mtry, because the overestimation is seen to depend on the
                  parameter mtry. The simulation-based and real-data based studies with metric predictor variables
                  performed in this paper show that the overestimation is largest in balanced settings and in settings
                  with few observations, a large number of predictor variables, small correlations between predictors
                  and weak effects. There was hardly any impact of the overestimation on tuning parameter
                  selection. However, although the prediction performance of random forests was not substantially
                  affected when using the out-of-bag error for tuning parameter selection in the present studies, one
                  cannot be sure that this applies to all future data. For settings with metric predictor variables it
                  is therefore strongly recommended to use stratified subsampling with sampling fractions that are
                  proportional to the class sizes for both tuning parameter selection and error estimation in random
                  forests. This yielded less biased estimates of the true prediction error. In unbalanced settings, in
                  which there is a strong interest in predicting observations from the smaller classes well, sampling
                  the same number of observations from each class is a promising alternative.},
  langid =	 {english},
  keywords =	 {Breast cancer,Colorectal cancer,Forecasting,Genomics,Normal distribution,Prostate cancer,Simulation
                  and modeling,Trees},
  file =	 {/home/dun280/Dropbox/zotero/Janitza_Hornung/Janitza_Hornung_2018_On the overestimation of random
                  forest’s out-of-bag error.pdf}
}

@Misc{		  jiangMultipleTestingTest2021,
  title =	 {Multiple Testing with Test Statistics Following Heavy-Tailed Distributions},
  author =	 {Jiang, Zhiwen},
  year =	 2021,
  journal =	 {Infoscience},
  number =	 {THESIS},
  publisher =	 {{EPFL}},
  doi =		 {10.5075/epfl-thesis-8102},
  abstract =	 {In multiple testing problems where the components come from a mixture model of noise and true effect,
                  we seek to first test for the existence of the non-zero components, and then identify the true
                  alternatives under a fixed significance level \$\textbackslash alpha\$. Two parameters, namely the
                  fraction of the non-null components \$\textbackslash varepsilon\$ and the size of the effects
                  \$\textbackslash mu\$, characterise the two-point mixture model under the global alternative. When the
                  number of hypotheses \$m\$ goes to infinity, we are interested in an asymptotic framework where the
                  fraction of the non-null components is vanishing, and the true effects need to be sizable to be
                  detected. Donoho and Jin give an explicit form of the asymptotic detectable boundary based on the
                  Gaussian mixture model under the classic calibration of the parameters of the mixture model. We prove
                  the analogous results for the Cauchy mixture distribution as an example heavy-tailed case. This
                  requires a different formulation of the parameters, which reflects the added difficulties. We also
                  propose a multiple testing procedure based on a filtering approach that can discover the true
                  alternatives.  Benjamini and Hochberg (BH) compare the observed \$p\$-values to a linear threshold
                  curve and reject the null hypotheses from the minimum up to the last up-crossing, and prove the false
                  discovery rate (FDR) is controlled. However, there is an intrinsic difference in heavy-tailed
                  settings. Were we to use the BH procedure we would get a highly variable positive false discovery rate
                  (pFDR). In our study we analyse the distribution of the \$p\$-values and devise a new multiple testing
                  procedure to combine the usual case and the heavy-tailed case based on the empirical properties of the
                  \$p\$-values. The filtering approach is designed to eliminate most \$p\$-values that are more likely
                  to be uniform, while preserving most of the true alternatives. Based on the filtered \$p\$-values, we
                  estimate the mode \$\textbackslash vartheta\$ and define the rejection region \$\textbackslash
                  mathscr\{R\}(\textbackslash vartheta, \textbackslash delta)=\textbackslash left[ \textbackslash
                  vartheta -\textbackslash delta/2, \textbackslash vartheta +\textbackslash delta/2 \textbackslash
                  right]\$ such that the most informative \$p\$-values are included. The length \$\textbackslash delta\$
                  is chosen by controlling the data-dependent estimation of FDR at a desired level.},
  howpublished = {https://infoscience.epfl.ch/record/283692},
  langid =	 {english},
  file =	 {/home/dun280/Dropbox/zotero/Jiang/Jiang_2021_Multiple testing with test statistics following
                  heavy-tailed distributions.pdf}
}

@Article{	  korthauerPracticalGuideMethods2019,
  title =	 {A Practical Guide to Methods Controlling False Discoveries in Computational Biology},
  author =	 {Korthauer, Keegan and Kimes, Patrick K. and Duvallet, Claire and Reyes, Alejandro and Subramanian,
                  Ayshwarya and Teng, Mingxiang and Shukla, Chinmay and Alm, Eric J. and Hicks, Stephanie C.},
  year =	 2019,
  month =	 jun,
  journal =	 {Genome Biology},
  volume =	 20,
  number =	 1,
  pages =	 118,
  issn =	 {1474-760X},
  doi =		 {10.1186/s13059-019-1716-1},
  abstract =	 {In high-throughput studies, hundreds to millions of hypotheses are typically tested. Statistical
                  methods that control the false discovery rate (FDR) have emerged as popular and powerful tools for
                  error rate control. While classic FDR methods use only p values as input, more modern FDR methods have
                  been shown to increase power by incorporating complementary information as informative covariates to
                  prioritize, weight, and group hypotheses.  However, there is currently no consensus on how the modern
                  methods compare to one another. We investigate the accuracy, applicability, and ease of use of two
                  classic and six modern FDR-controlling methods by performing a systematic benchmark comparison using
                  simulation studies as well as six case studies in computational biology.},
  keywords =	 {ChIP-seq,False discovery rate,Gene set analysis,GWAS,Microbiome,Multiple hypothesis
                  testing,RNA-seq,ScRNA-seq},
  file =	 {/home/dun280/Dropbox/zotero/Korthauer et al/Korthauer et al_2019_A practical guide to methods
                  controlling false discoveries in computational.pdf}
}

@Article{	  limma.2015,
  author =	 {Matthew E Ritchie and Belinda Phipson and Di Wu and Yifang Hu and Charity W Law and Wei Shi and Gordon
                  K Smyth},
  title =	 {{limma} powers differential expression analyses for {RNA}-sequencing and microarray studies},
  journal =	 {Nucleic Acids Research},
  year =	 2015,
  volume =	 43,
  number =	 7,
  pages =	 {e47},
  doi =		 {10.1093/nar/gkv007}
}

@inproceedings{lulliReForeStRandomForests2017a,
  title =	 {{{ReForeSt}}: {{Random Forests}} in {{Apache Spark}}},
  shorttitle =	 {{{ReForeSt}}},
  booktitle =	 {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2017},
  author =	 {Lulli, Alessandro and Oneto, Luca and Anguita, Davide},
  editor =	 {Lintas, Alessandra and Rovetta, Stefano and Verschure, Paul F.M.J. and Villa, Alessandro E.P.},
  year =	 2017,
  series =	 {Lecture {{Notes}} in {{Computer Science}}},
  pages =	 {331--339},
  publisher =	 {{Springer International Publishing}},
  address =	 {{Cham}},
  doi =		 {10.1007/978-3-319-68612-7_38},
  abstract =	 {Random Forests (RF) of tree classifiers are a popular ensemble method for classification. RF are
                  usually preferred with respect to other classification techniques because of their limited
                  hyperparameter sensitivity, high numerical robustness, native capacity of dealing with numerical and
                  categorical features, and effectiveness in many real world classification problems. In this work we
                  present ReForeSt, a Random Forests Apache Spark implementation which is easier to tune, faster, and
                  less memory consuming with respect to MLlib, the de facto standard Apache Spark machine learning
                  library. We perform an extensive comparison between ReForeSt and MLlib by taking advantage of the
                  Google Cloud Platform (https://cloud.google.com). In particular, we test ReForeSt and MLlib with
                  different library settings, on different real world datasets, and with a different number of machines
                  equipped with different number of cores. Results confirm that ReForeSt outperforms MLlib in all the
                  above mentioned aspects. ReForeSt is made publicly available via GitHub
                  (https://github.com/alessandrolulli/reforest).},
  isbn =	 {978-3-319-68612-7},
  langid =	 {english},
  keywords =	 {Apache Spark,Open source software,Random Forests},
  file =	 {/home/dun280/local/share/zotero/storage/L7E9J7WI/Lulli et al_2017_ReForeSt.pdf}
}

@Article{	  lundbergConsistentIndividualizedFeature2019,
  title =	 {Consistent {{Individualized Feature Attribution}} for {{Tree Ensembles}}},
  author =	 {Lundberg, Scott M. and Erion, Gabriel G. and Lee, Su-In},
  year =	 2019,
  month =	 mar,
  abstract =	 {Interpreting predictions from tree ensemble methods such as gradient boosting machines and random
                  forests is important, yet feature attribution for trees is often heuristic and not individualized for
                  each prediction. Here we show that popular feature attribution methods are inconsistent, meaning they
                  can lower a feature's assigned importance when the true impact of that feature actually
                  increases. This is a fundamental problem that casts doubt on any comparison between features. To
                  address it we turn to recent applications of game theory and develop fast exact tree solutions for
                  SHAP (SHapley Additive exPlanation) values, which are the unique consistent and locally accurate
                  attribution values. We then extend SHAP values to interaction effects and define SHAP interaction
                  values. We propose a rich visualization of individualized feature attributions that improves over
                  classic attribution summaries and partial dependence plots, and a unique "supervised" clustering
                  (clustering based on feature attributions). We demonstrate better agreement with human intuition
                  through a user study, exponential improvements in run time, improved clustering performance, and
                  better identification of influential features. An implementation of our algorithm has also been merged
                  into XGBoost and LightGBM, see http://github.com/slundberg/shap for details.},
  archiveprefix ={arXiv},
  eprint =	 {1802.03888},
  eprinttype =	 {arxiv},
  file =	 {/home/dun280/Dropbox/zotero/arXiv1802.03888 [cs, stat]/2019/Lundberg et al_2019_Consistent
                  Individualized Feature Attribution for Tree Ensembles.pdf},
  journal =	 {arXiv:1802.03888 [cs, stat]},
  keywords =	 {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@Article{	  lundbergLocalExplanationsGlobal2020,
  title =	 {From Local Explanations to Global Understanding with Explainable {{AI}} for Trees},
  author =	 {Lundberg, Scott M. and Erion, Gabriel and Chen, Hugh and DeGrave, Alex and Prutkin, Jordan M. and
                  Nair, Bala and Katz, Ronit and Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In},
  year =	 2020,
  month =	 jan,
  volume =	 2,
  pages =	 {56--67},
  publisher =	 {{Nature Publishing Group}},
  issn =	 {2522-5839},
  doi =		 {10.1038/s42256-019-0138-9},
  abstract =	 {Tree-based machine learning models such as random forests, decision trees and gradient boosted trees
                  are popular nonlinear predictive models, yet comparatively little attention has been paid to
                  explaining their predictions.  Here we improve the interpretability of tree-based models through three
                  main contributions. (1) A polynomial time algorithm to compute optimal explanations based on game
                  theory. (2) A new type of explanation that directly measures local feature interaction effects. (3) A
                  new set of tools for understanding global model structure based on combining many local explanations
                  of each prediction. We apply these tools to three medical machine learning problems and show how
                  combining many high-quality local explanations allows us to represent global structure while retaining
                  local faithfulness to the original model. These tools enable us to (1) identify high-magnitude but
                  low-frequency nonlinear mortality risk factors in the US population, (2) highlight distinct population
                  subgroups with shared risk characteristics, (3) identify nonlinear interaction effects among risk
                  factors for chronic kidney disease and (4) monitor a machine learning model deployed in a hospital by
                  identifying which features are degrading the model's performance over time. Given the popularity of
                  tree-based machine learning models, these improvements to their interpretability have implications
                  across a broad set of domains.},
  copyright =	 {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  file =	 {/home/dun280/Dropbox/zotero/Nature Machine Intelligence/2020/Lundberg et al_2020_From local
                  explanations to global understanding with explainable AI for trees.pdf},
  journal =	 {Nature Machine Intelligence},
  language =	 {en},
  number =	 1
}

@Article{	  lundbergUnifiedApproachInterpreting2017,
  title =	 {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  author =	 {Lundberg, Scott M. and Lee, Su-In},
  year =	 2017,
  volume =	 30,
  pages =	 {4765--4774},
  file =	 {/home/dun280/Dropbox/zotero/Advances in Neural Information Processing Systems/2017/Lundberg_Lee_2017_A
                  Unified Approach to Interpreting Model Predictions.pdf},
  journal =	 {Advances in Neural Information Processing Systems},
  language =	 {en}
}

@Article{	  pmid11207349,
  author =	 "Hedenfalk, I. and Duggan, D. and Chen, Y. and Radmacher, M. and Bittner, M. and Simon, R. and Meltzer,
                  P. and Gusterson, B. and Esteller, M. and Kallioniemi, O. P. and Wilfond, B. and Borg, A. and Trent,
                  J. and Raffeld, M. and Yakhini, Z. and Ben-Dor, A. and Dougherty, E. and Kononen, J. and Bubendorf,
                  L. and Fehrle, W. and Pittaluga, S. and Gruvberger, S. and Loman, N. and Johannsson, O. and Olsson,
                  H. and Sauter, G. ",
  title =	 "{{G}ene-expression profiles in hereditary breast cancer}",
  journal =	 "N. Engl. J. Med.",
  year =	 2001,
  volume =	 344,
  number =	 8,
  pages =	 "539--548",
  month =	 "Feb",
  abstract =	 {Many cases of hereditary breast cancer are due to mutations in either the BRCA1 or the BRCA2 gene. The
                  histopathological changes in these cancers are often characteristic of the mutant gene. We
                  hypothesized that the genes expressed by these two types of tumors are also distinctive, perhaps
                  allowing us to identify cases of hereditary breast cancer on the basis of gene-expression profiles.\\
                  RNA from samples of primary tumor from seven carriers of the BRCA1 mutation, seven carriers of the
                  BRCA2 mutation, and seven patients with sporadic cases of breast cancer was compared with a microarray
                  of 6512 complementary DNA clones of 5361 genes. Statistical analyses were used to identify a set of
                  genes that could distinguish the BRCA1 genotype from the BRCA2 genotype.\\ Permutation analysis of
                  multivariate classification functions established that the gene-expression profiles of tumors with
                  BRCA1 mutations, tumors with BRCA2 mutations, and sporadic tumors differed significantly from each
                  other. An analysis of variance between the levels of gene expression and the genotype of the samples
                  identified 176 genes that were differentially expressed in tumors with BRCA1 mutations and tumors with
                  BRCA2 mutations. Given the known properties of some of the genes in this panel, our findings indicate
                  that there are functional differences between breast tumors with BRCA1 mutations and those with BRCA2
                  mutations.\\ Significantly different groups of genes are expressed by breast cancers with BRCA1
                  mutations and breast cancers with BRCA2 mutations. Our results suggest that a heritable mutation
                  influences the gene-expression profile of the cancer.},
  note =	 {[DOI:\href{https://dx.doi.org/10.1056/NEJM200102223440801}{10.1056/NEJM200102223440801}]
                  [PubMed:\href{https://www.ncbi.nlm.nih.gov/pubmed/11207349}{11207349}] }
}

@Manual{randomForestExplainer.2020,
  title =	 {randomForestExplainer: Explaining and Visualizing Random Forests in Terms of Variable Importance},
  author =	 {Aleksandra Paluszynska and Przemyslaw Biecek and Yue Jiang},
  year =	 2020,
  note =	 {R package version 0.10.1},
  url =		 {https://CRAN.R-project.org/package=randomForestExplainer}
}

@article{rudinInterpretableMachineLearning2021,
  title =	 {Interpretable {{Machine Learning}}: Fundamental {{Principles}} and 10 {{Grand Challenges}}},
  shorttitle =	 {Interpretable {{Machine Learning}}},
  author =	 {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong,
                  Chudi},
  year =	 2021,
  month =	 mar,
  journal =	 {arXiv:2103.11251 [cs, stat]},
  eprint =	 {2103.11251},
  eprinttype =	 {arxiv},
  primaryclass = {cs, stat},
  abstract =	 {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In
                  this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings
                  that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in
                  interpretable machine learning and provide history and background on each problem. Some of these
                  problems are classically important, and some are recent problems that have arisen in the last few
                  years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2)
                  Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage
                  sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and
                  matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6)
                  Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction
                  for data visualization; (8) Machine learning models that can incorporate physics and other generative
                  or causal constraints; (9) Characterization of the "Rashomon set" of good models; and (10)
                  Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians
                  and computer scientists interested in working in interpretable machine learning.},
  archiveprefix ={arXiv},
  keywords =	 {68T01,Computer Science - Machine Learning,I.2.6,Statistics - Machine Learning},
  file =	 {/home/dun280/Dropbox/zotero/Rudin et al/Rudin et al_2021_Interpretable Machine Learning.pdf}
}

@Article{	  storeyStatisticalSignificanceGenomewide2003,
  title =	 {Statistical Significance for Genomewide Studies},
  author =	 {Storey, John D. and Tibshirani, Robert},
  year =	 2003,
  month =	 aug,
  journal =	 {Proceedings of the National Academy of Sciences},
  volume =	 100,
  number =	 16,
  pages =	 {9440--9445},
  publisher =	 {{National Academy of Sciences}},
  issn =	 {0027-8424, 1091-6490},
  doi =		 {10.1073/pnas.1530509100},
  abstract =	 {With the increase in genomewide experiments and the sequencing of multiple genomes, the analysis of
                  large data sets has become commonplace in biology. It is often the case that thousands of features in
                  a genomewide data set are tested against some null hypothesis, where a number of features are expected
                  to be significant. Here we propose an approach to measuring statistical significance in these
                  genomewide studies based on the concept of the false discovery rate. This approach offers a sensible
                  balance between the number of true and false positives that is automatically calibrated and easily
                  interpreted. In doing so, a measure of statistical significance called the q value is associated with
                  each tested feature. The q value is similar to the well known p value, except it is a measure of
                  significance in terms of the false discovery rate rather than the false positive rate. Our approach
                  avoids a flood of false positive results, while offering a more liberal criterion than what has been
                  used in genome scans for linkage.},
  chapter =	 {Physical Sciences},
  copyright =	 {Copyright \textcopyright{} 2003, The National Academy of Sciences},
  langid =	 {english},
  pmid =	 12883005,
  keywords =	 {false discovery rates,genomics,multiple hypothesis testing,q values},
  file =	 {/home/dun280/Dropbox/zotero/Storey_Tibshirani/Storey_Tibshirani_2003_Statistical significance for
                  genomewide studies.pdf}
}

@article{tolosiClassificationCorrelatedFeatures2011,
  title =	 {Classification with Correlated Features: Unreliability of Feature Ranking and Solutions},
  shorttitle =	 {Classification with Correlated Features},
  author =	 {Tolosi, Laura and Lengauer, Thomas},
  year =	 2011,
  month =	 jul,
  journal =	 {Bioinformatics (Oxford, England)},
  volume =	 27,
  number =	 14,
  pages =	 {1986--1994},
  issn =	 {1367-4811},
  doi =		 {10.1093/bioinformatics/btr300},
  abstract =	 {MOTIVATION: Classification and feature selection of genomics or transcriptomics data is often hampered
                  by the large number of features as compared with the small number of samples available. Moreover,
                  features represented by probes that either have similar molecular functions (gene expression analysis)
                  or genomic locations (DNA copy number analysis) are highly correlated. Classical model selection
                  methods such as penalized logistic regression or random forest become unstable in the presence of high
                  feature correlations. Sophisticated penalties such as group Lasso or fused Lasso can force the models
                  to assign similar weights to correlated features and thus improve model stability and
                  interpretability. In this article, we show that the measures of feature relevance corresponding to the
                  above-mentioned methods are biased such that the weights of the features belonging to groups of
                  correlated features decrease as the sizes of the groups increase, which leads to incorrect model
                  interpretation and misleading feature ranking. RESULTS: With simulation experiments, we demonstrate
                  that Lasso logistic regression, fused support vector machine, group Lasso and random forest models
                  suffer from correlation bias. Using simulations, we show that two related methods for group selection
                  based on feature clustering can be used for correcting the correlation bias. These techniques also
                  improve the stability and the accuracy of the baseline models. We apply all methods investigated to a
                  breast cancer and a bladder cancer arrayCGH dataset and in order to identify copy number aberrations
                  predictive of tumor phenotype. AVAILABILITY: R code can be found at:
                  http://www.mpi-inf.mpg.de/\textasciitilde laura/Clustering.r.},
  langid =	 {english},
  pmid =	 21576180,
  keywords =	 {Breast Neoplasms,Cluster Analysis,Comparative Genomic Hybridization,Female,Genomics,Humans,Logistic
                  Models,Models; Biological,Models; Molecular,Neoplasms,Solutions,Statistics as Topic,Urinary Bladder
                  Neoplasms},
  file =	 {/home/dun280/Dropbox/zotero/Tolosi_Lengauer/Tolosi_Lengauer_2011_Classification with correlated
                  features.pdf}
}

@article{witteGenomewideAssociationStudies2010,
  title =	 {Genome-Wide Association Studies and Beyond},
  author =	 {Witte, John S.},
  year =	 2010,
  journal =	 {Annual Review of Public Health},
  volume =	 31,
  pages =	 {9-20 4 p following 20},
  issn =	 {1545-2093},
  doi =		 {10.1146/annurev.publhealth.012809.103723},
  abstract =	 {Genome-wide association studies (GWAS) provide an important avenue for undertaking an agnostic
                  evaluation of the association between common genetic variants and risk of disease. Recent advances in
                  our understanding of human genetic variation and the technology to measure such variation have made
                  GWAS feasible. Over the past few years a multitude of GWAS have identified and replicated many
                  associated variants. These findings are enriching our knowledge about the genetic basis of disease and
                  leading some to advocate using GWA study results for genetic testing. For many of the GWA study
                  results, however, the underlying mechanisms remain unclear and the findings explain only a limited
                  amount of heritability. These issues may be clarified by more detailed investigations, including
                  analyses of less common variants, sequence-level data, and environmental exposures. Such studies
                  should help clarify the potential value of genetic testing to the public's health.},
  langid =	 {english},
  pmcid =	 {PMC3997166},
  pmid =	 20235850,
  keywords =	 {DNA Copy Number Variations,Genetic Testing,Genome-Wide Association Study,Humans,Linkage
                  Disequilibrium,Polymorphism; Single Nucleotide,Population Surveillance},
  file =	 {/home/dun280/Dropbox/zotero/Witte/Witte_2010_Genome-wide association studies and beyond.pdf}
}


@article{ashourApproximateSkewNormal2010,
  title = {Approximate Skew Normal Distribution},
  author = {Ashour, Samir K. and Abdel-hameed, Mahmood A.},
  year = {2010},
  journal = {Journal of Advanced Research},
  journaltitle = {Journal of Advanced Research},
  shortjournal = {Journal of Advanced Research},
  volume = {1},
  number = {4},
  pages = {341--350},
  issn = {2090-1232},
  doi = {10.1016/j.jare.2010.06.004},
  url = {https://www.sciencedirect.com/science/article/pii/S209012321000069X},
  urldate = {2022-04-28},
  abstract = {We propose a new approximate skew normal distribution, it is easy to calculate, convenient, mathematically tractable and is in a closed form. It is particularly useful when the probability density function occurs in an expression to be used for further mathematical derivation or in programs for the skew normal distribution. Also, we propose approximate first moment second moment and variance to the skew normal distribution. A numerical comparison between exact and approximate values of pdf and cdf of the skew normal distribution is carried out.},
  langid = {english},
  keywords = {Approximation,Cumulative distribution function,Skew normal distribution,Skewness},
  file = {/home/dun280/Dropbox/zotero/Journal of Advanced Research/2010/Ashour_Abdel-hameed_2010_Approximate skew normal distribution.pdf}
}


@Manual{Pomona.2022,
    title = {Pomona: Identification of relevant variables in omics data sets using Random Forests},
    author = {Cesaire Fouodo},
    year = {2022},
    note = {R package version 1.0.2},
  }

@Manual{vita.2015,
    title = {vita: Variable Importance Testing Approaches},
    author = {Ender Celik},
    year = {2015},
    note = {R package version 1.0.0},
    url = {https://CRAN.R-project.org/package=vita},
  }

@misc{dunneThresholdingGiniVariable2022,
  title = {Thresholding {{Gini Variable Importance}} with a Single Trained {{Random Forest}}: {{An Empirical Bayes Approach}}},
  shorttitle = {Thresholding {{Gini Variable Importance}} with a Single Trained {{Random Forest}}},
  author = {Dunne, Robert and Reguant, Roc and Ramarao-Milne, Priya and Szul, Piotr and Sng, Letitia and Lundberg, Mischa and Twine, Natalie A. and Bauer, Denis C.},
  date = {2022-10-11},
  pages = {2022.04.06.487300},
  publisher = {{bioRxiv}},
  doi = {10.1101/2022.04.06.487300},
  url = {https://www.biorxiv.org/content/10.1101/2022.04.06.487300v2},
  urldate = {2023-01-25},
  abstract = {Background Random Forests (RF) are a widely used modelling tool, enabling feature-selection via a variable importance measure. For this, a threshold is required that separates label-associated features from false positives. In the absence of a good understanding of the characteristics of the variable importance measures, current approaches attempt to select features by training multiple random forest to generate statistical power via a permutation null, employ recursive feature elimination or a combination of both. However, for high-dimensional datasets, such as genome data with millions of variables, this is computationally infeasible. Method Here we present RFlocalfdr, a statistical approach to a threshold that identifies which features are significantly associated with the prediction label and reduces false positives. It builds on the empirical Bayes argument of Efron (2005) and models the variable importance as mixture of two distributions – null and non-null “genes.” Result We demonstrate on synthetic data that RFlocalfdr has an equivalent performance to computationally more intensive approaches. But unlike these methods, RFlocalfdr successfully thresholds a 6 Million-feature dataset of 10,000 samples in real-time, being up to 100 times faster than other tools. RFlocalfdr can be applied to the output of any RF implementation returning variable importance and counts and we demonstrate its application to ranger and VariantSpark. Conclusion RFlocalfdr allows for robust feature selection by placing a confidence value on the predicted importance score. It does so without repeated fitting of the RF or the use of additional shadow variables and is thus usable for data sets with very large numbers of variables.},
  langid = {english},
  file = {/home/dun280/Dropbox/zotero/bioRxiv/2022/Dunne et al_2022_Thresholding Gini Variable Importance with a single trained Random Forest.pdf}
}

@Manual{locfdr.2015,
    title = {locfdr: Computes Local False Discovery Rates},
    author = {Bradley Efron and Brit Turnbull and Balasubramanian Narasimhan},
    year = {2015},
    note = {R package version 1.1-8},
    url = {https://CRAN.R-project.org/package=locfdr},
  }

